{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 10\n",
    "conversion_constant = (2.0*16.0)/8192\n",
    "folder = \"labeled_dataset_downsample_200_filter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_class(filename):\n",
    "    match = re.search(r'benchy_\\d+_(.*?)\\.parquet\\.gzip', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_using_rolling(df,window=44,step=22,columns=[\"accel_x\",\"accel_y\",\"accel_z\"]):\n",
    "    for column in set(columns):\n",
    "        df[column] = df[column]*conversion_constant\n",
    "    rolling = df[columns].rolling(window=window,step=step)\n",
    "    return np.array(list(rolling)[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_extraction(dataset,label):\n",
    "    p_data = np.empty((0,*dataset.shape[1:]))\n",
    "\n",
    "    for sample in dataset:\n",
    "        # Perform 2D Discrete Fourier Transform (DFT)\n",
    "        dft_image = np.fft.fft2(sample)\n",
    "\n",
    "        # Compute the magnitude spectrum (for visualization)\n",
    "        magnitude_spectrum = np.log10(np.abs(dft_image)+1)\n",
    "        p_data = np.vstack((p_data,np.reshape(magnitude_spectrum,(1,*magnitude_spectrum.shape))\n",
    "    ))\n",
    "\n",
    "    return p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(dataset,label):\n",
    "    p_data = np.empty((0,9))\n",
    "\n",
    "    for sample in dataset:\n",
    "        pca = PCA(n_components=1)\n",
    "        pca.fit(sample)\n",
    "        data = np.hstack([np.mean(sample,axis=0), np.std(sample,axis=0), pca.components_[0]])\n",
    "        p_data = np.vstack((p_data,data))\n",
    "\n",
    "    label_column = np.ones((p_data.shape[0], 1))*label\n",
    "    p_data = np.hstack((p_data,label_column))\n",
    "    return p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dataset,label):\n",
    "    p_data = np.empty((0,9))\n",
    "\n",
    "    for sample in dataset:\n",
    "        pca = PCA(n_components=1)\n",
    "        pca.fit(sample)\n",
    "        data = np.hstack([np.mean(sample,axis=0), np.std(sample,axis=0), pca.components_[0]])\n",
    "        p_data = np.vstack((p_data,data))\n",
    "    \n",
    "    label_column = np.ones((p_data.shape[0], 1))*label\n",
    "    p_data = np.hstack((p_data,label_column))\n",
    "    return p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "list_classes = set()\n",
    "for file in Path(folder).glob(\"benchy_*\"):\n",
    "    list_classes.add(extract_class(file.name))\n",
    "print(list_classes)\n",
    "\n",
    "class_index = {'healthy':0, 'temp_220':1, 'temp_230':2, 'nozzle_03':3,'nozzle_02':4, 'loose_head':5}\n",
    "print(class_index)\n",
    "with open(Path(folder)/\"class_index.json\", 'w') as json_file:\n",
    "    json.dump(class_index, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image dataset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.empty((0, timesteps, 44, 4))\n",
    "y_data = np.empty((0, 1))\n",
    "\n",
    "def process_file(file):\n",
    "    df = pd.read_parquet(file)\n",
    "    data = window_using_rolling(df, columns=[\"accel_x\", \"accel_y\", \"accel_z\", \"accel_x\"])\n",
    "    unique = df[\"class\"].unique()\n",
    "    if len(unique) != 1:\n",
    "        print(f\"failure: {file} - {unique}\")\n",
    "        return None\n",
    "    else:\n",
    "        unique = unique[0]\n",
    "    \n",
    "    processed_data = image_extraction(data, class_index[unique])\n",
    "    processed_data2 = processed_data[processed_data.shape[0] % timesteps:, :]\n",
    "    reshaped_data = processed_data2.reshape((\n",
    "        processed_data2.shape[0] // timesteps,\n",
    "        timesteps,\n",
    "        *processed_data2.shape[1:],\n",
    "    ))\n",
    "    y_data_temp = np.ones((reshaped_data.shape[0], 1)) * class_index[unique]\n",
    "    return reshaped_data, y_data_temp\n",
    "\n",
    "def collect_results(result):\n",
    "    global x_data, y_data\n",
    "    if result is not None:\n",
    "        reshaped_data, y_data_temp = result\n",
    "        x_data = np.vstack((x_data, reshaped_data))\n",
    "        y_data = np.vstack((y_data, y_data_temp))\n",
    "\n",
    "with Pool(cpu_count()) as pool:\n",
    "    files = list(Path(folder).glob(\"benchy_*\"))\n",
    "    for result in tqdm(pool.imap_unordered(process_file, files), total=len(files)):\n",
    "        collect_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(Path(folder)/\"processed_labeled_image_data\",x_data,fix_imports=False)\n",
    "np.save(Path(folder)/\"processed_labeled_image_result\",y_data,fix_imports=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accel_x = np.array([])\n",
    "accel_y = np.array([])\n",
    "accel_z = np.array([])\n",
    "counter = dict()\n",
    "\n",
    "for file in tqdm(list(Path(folder).glob(\"benchy_*\"))):\n",
    "    file_class = extract_class(file.name)\n",
    "    df = pd.read_parquet(file)\n",
    "    accel_x = np.concatenate([accel_x,df[\"accel_x\"]*conversion_constant])\n",
    "    accel_y = np.concatenate([accel_y,df[\"accel_y\"]*conversion_constant])\n",
    "    accel_z = np.concatenate([accel_z,df[\"accel_z\"]*conversion_constant])\n",
    "\n",
    "mean_accel_x = np.mean(accel_x)\n",
    "mean_accel_y = np.mean(accel_y)\n",
    "mean_accel_z = np.mean(accel_z)\n",
    "\n",
    "std_accel_x = np.std(accel_x)\n",
    "std_accel_y = np.std(accel_y)\n",
    "std_accel_z = np.std(accel_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 10\n",
    "x_data = np.empty((0, timesteps, 9))\n",
    "y_data = np.empty((0, 1))\n",
    "\n",
    "def process_file(file):\n",
    "    df = pd.read_parquet(file)\n",
    "    \n",
    "    # Normalize accelerometer data\n",
    "    df[\"accel_x\"] = (df[\"accel_x\"] * conversion_constant - mean_accel_x) / std_accel_x\n",
    "    df[\"accel_y\"] = (df[\"accel_y\"] * conversion_constant - mean_accel_y) / std_accel_y\n",
    "    df[\"accel_z\"] = (df[\"accel_z\"] * conversion_constant - mean_accel_z) / std_accel_z\n",
    "\n",
    "    # Apply rolling window function\n",
    "    data = window_using_rolling(df)\n",
    "    \n",
    "    unique = df[\"class\"].unique()\n",
    "    if len(unique) != 1:\n",
    "        print(f\"failure: {file} - {unique}\")\n",
    "        return None, None\n",
    "    unique = unique[0]\n",
    "\n",
    "    # Feature extraction\n",
    "    processed_data = feature_extraction(data, class_index[unique])\n",
    "\n",
    "    # Reshape the data\n",
    "    processed_data2 = processed_data[processed_data.shape[0] % timesteps:, :-1]\n",
    "    reshaped_data = processed_data2.reshape((\n",
    "        processed_data2.shape[0] // timesteps,\n",
    "        timesteps,\n",
    "        *processed_data2.shape[1:],\n",
    "    ))\n",
    "\n",
    "    return reshaped_data, np.ones((reshaped_data.shape[0], 1)) * class_index[unique]\n",
    "\n",
    "def collect_results(result):\n",
    "    global x_data, y_data\n",
    "    if result is not None:\n",
    "        reshaped_data, y_data_temp = result\n",
    "        x_data = np.vstack((x_data, reshaped_data))\n",
    "        y_data = np.vstack((y_data, y_data_temp))\n",
    "\n",
    "# Create a pool of workers equal to the number of CPU cores\n",
    "with Pool(cpu_count()) as pool:\n",
    "    files = list(Path(folder).glob(\"benchy_*\"))\n",
    "    futures = [pool.apply_async(process_file, (file,)) for file in files]\n",
    "    \n",
    "    for future in tqdm(futures):\n",
    "        result = future.get()\n",
    "        collect_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(Path(folder)/\"processed_labeled_filtered_normalized_data\",x_data,fix_imports=False)\n",
    "np.save(Path(folder)/\"processed_labeled_filtered_normalized_result\",y_data,fix_imports=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_robotics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
