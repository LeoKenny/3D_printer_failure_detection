{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Bidirectional,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    MaxPooling1D,\n",
    "    TimeDistributed,\n",
    "    Dropout\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_class(filename):\n",
    "    match = re.search(r'benchy_\\d+_(.*?)\\.parquet\\.gzip', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dataset,label):\n",
    "    p_data = np.empty((0,9))\n",
    "\n",
    "    for sample in dataset:\n",
    "        pca = PCA(n_components=1)\n",
    "        pca.fit(sample)\n",
    "        data = np.hstack([np.mean(sample,axis=0), np.std(sample,axis=0), pca.components_[0]])\n",
    "        p_data = np.vstack((p_data,data))\n",
    "    \n",
    "    label_column = np.ones((p_data.shape[0], 1))*label\n",
    "    p_data = np.hstack((p_data,label_column))\n",
    "    return p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dataset,label):\n",
    "    p_data = np.empty((0,9))\n",
    "\n",
    "    for sample in dataset:\n",
    "        pca = PCA(n_components=1)\n",
    "        pca.fit(sample)\n",
    "        data = np.hstack([np.mean(sample,axis=0), np.std(sample,axis=0), pca.components_[0]])\n",
    "        p_data = np.vstack((p_data,data))\n",
    "    \n",
    "    label_column = np.ones((p_data.shape[0], 1))*label\n",
    "    p_data = np.hstack((p_data,label_column))\n",
    "    return p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_using_rolling(df,window=44,step=22,columns=[\"accel_x\",\"accel_y\",\"accel_z\"]):\n",
    "    rolling = df[columns].rolling(window=window,step=step)\n",
    "    return np.array(list(rolling)[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 3200\n",
    "print_resolution = 0.1\n",
    "print_speed = 60\n",
    "minimum_print_steps = 2\n",
    "min_print_window = minimum_print_steps*print_resolution/print_speed\n",
    "samples_per_window = min_print_window*sample_rate\n",
    "print(f\"Minimum print window: {min_print_window}\")\n",
    "print(f\"Samples per window: {samples_per_window}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "list_classes = set()\n",
    "for file in Path(\"downsampled_200_filter\").glob(\"benchy_*\"):\n",
    "    list_classes.add(extract_class(file.name))\n",
    "print(list_classes)\n",
    "\n",
    "class_index = dict()\n",
    "for i,class_name in enumerate(list_classes):\n",
    "    class_index[class_name] = int(i)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data = np.empty((0,10))\n",
    "\n",
    "# for file in list(Path(\"downsampled_200\").glob(\"benchy_*\")):\n",
    "#     file_class = extract_class(file.name)\n",
    "#     df = pd.read_parquet(file)\n",
    "#     data = window_using_rolling(df)\n",
    "#     processed_data = process_data(data,class_index[file_class])\n",
    "#     full_data = np.vstack((full_data,processed_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = np.empty((0,10))\n",
    "\n",
    "for file in list(Path(\"downsampled_200\").glob(\"benchy_*\")):\n",
    "    file_class = extract_class(file.name)\n",
    "    df = pd.read_parquet(file)\n",
    "    df[\"accel_x\"] = (df[\"accel_x\"]-df[\"accel_x\"].mean())/df[\"accel_x\"].std()\n",
    "    df[\"accel_y\"] = (df[\"accel_y\"]-df[\"accel_y\"].mean())/df[\"accel_y\"].std()\n",
    "    df[\"accel_z\"] = (df[\"accel_z\"]-df[\"accel_z\"].mean())/df[\"accel_z\"].std()\n",
    "    data = window_using_rolling(df)\n",
    "    processed_data = process_data(data,class_index[file_class])\n",
    "    full_data = np.vstack((full_data,processed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"processed_labeled_normalized_data\",full_data,fix_imports=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_labeled_normalized_data.npy', 'rb') as f:\n",
    "    loaded_data = np.load(f)\n",
    "\n",
    "x_train = loaded_data[:,:9]\n",
    "y_train = loaded_data[:,9]\n",
    "x_train = x_train.reshape((x_train.shape[0],1,x_train.shape[1]))\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "y_train_categorical = to_categorical(y_train,num_classes=np.unique(y_train).shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add LSTM layer with 100 units\n",
    "model.add(LSTM(100, input_shape=(1, 9), return_sequences=False))\n",
    "\n",
    "# Add Dropout layer\n",
    "model.add(Dropout(0.2))  # Adjust dropout rate as needed\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"categorical_accuracy\",\"accuracy\"])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_vanilla_model.keras',       # Filepath to save the model\n",
    "    monitor='val_accuracy',            # Monitor validation accuracy\n",
    "    save_best_only=True,               # Save only the best model\n",
    "    save_weights_only=False,           # Save the full model (architecture + weights)\n",
    "    mode='max',                        # Save when the monitored quantity is maximized\n",
    "    verbose=1                          # Verbosity mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train_categorical,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_history_vanilla.json', 'w') as f:\n",
    "    json.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_train)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_train_classes = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Calculate precision and recall for multi-class classification\n",
    "precision = precision_score(y_train_classes, y_pred_classes, average='macro')\n",
    "recall = recall_score(y_train_classes, y_pred_classes, average='macro')\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_stacked = Sequential()\n",
    "\n",
    "# Add LSTM layer with 100 units\n",
    "model_stacked.add(LSTM(50, input_shape=(1, 9), return_sequences=True))\n",
    "model_stacked.add(LSTM(50))\n",
    "\n",
    "# Add Dropout layer\n",
    "model_stacked.add(Dropout(0.2))  # Adjust dropout rate as needed\n",
    "\n",
    "# Add output layer\n",
    "model_stacked.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile the model_stacked\n",
    "model_stacked.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"categorical_accuracy\",\"accuracy\"])\n",
    "\n",
    "# Summary of the model_stacked\n",
    "model_stacked.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_2_stack_model.keras',# Filepath to save the model\n",
    "    monitor='val_accuracy',            # Monitor validation accuracy\n",
    "    save_best_only=True,               # Save only the best model\n",
    "    save_weights_only=False,           # Save the full model (architecture + weights)\n",
    "    mode='max',                        # Save when the monitored quantity is maximized\n",
    "    verbose=1                          # Verbosity mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "history2 = model_stacked.fit(\n",
    "    x_train,\n",
    "    y_train_categorical,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_history_stacked.json', 'w') as f:\n",
    "    json.dump(history2.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_stacked.predict(x_train)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_train_classes = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Calculate precision and recall for multi-class classification\n",
    "precision = precision_score(y_train_classes, y_pred_classes, average='macro')\n",
    "recall = recall_score(y_train_classes, y_pred_classes, average='macro')\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bidirectional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_bi = Sequential()\n",
    "\n",
    "# Add LSTM layer with 100 units\n",
    "model_bi.add(Bidirectional(LSTM(units=100), input_shape=(1, 9)))\n",
    "\n",
    "# Add Dropout layer\n",
    "model_bi.add(Dropout(0.2))  # Adjust dropout rate as needed\n",
    "\n",
    "# Add output layer\n",
    "model_bi.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile the model_bi\n",
    "model_bi.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"categorical_accuracy\",\"accuracy\"])\n",
    "\n",
    "# Summary of the model_bi\n",
    "model_bi.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_bidirectional_model.keras',# Filepath to save the model\n",
    "    monitor='val_accuracy',            # Monitor validation accuracy\n",
    "    save_best_only=True,               # Save only the best model\n",
    "    save_weights_only=False,           # Save the full model (architecture + weights)\n",
    "    mode='max',                        # Save when the monitored quantity is maximized\n",
    "    verbose=1                          # Verbosity mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "history3 = model_bi.fit(\n",
    "    x_train,\n",
    "    y_train_categorical,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_history_bi.json', 'w') as f:\n",
    "    json.dump(history3.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_bi.predict(x_train)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_train_classes = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Calculate precision and recall for multi-class classification\n",
    "precision = precision_score(y_train_classes, y_pred_classes, average='macro')\n",
    "recall = recall_score(y_train_classes, y_pred_classes, average='macro')\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
